[Click here](../README.md) to view the README.

## Design and implementation

The design of this application is minimalistic to get started with code examples on PSOC&trade; Edge MCU devices. All PSOC&trade; Edge E84 MCU applications have a dual-CPU three-project structure to develop code for the CM33 and CM55 cores. The CM33 core has two separate projects for the secure processing environment (SPE) and non-secure processing environment (NSPE). A project folder consists of various subfolders, each denoting a specific aspect of the project. The three project folders are as follows:

**Table 1. Application projects**

Project | Description
--------|------------------------
*proj_cm33_s* | Project for CM33 secure processing environment (SPE)
*proj_cm33_ns* | Project for CM33 non-secure processing environment (NSPE)
*proj_cm55* | CM55 project

<br>

In this code example, at device reset, the secure boot process starts from the ROM boot with the secure enclave (SE) as the root of trust (RoT). From the secure enclave, the boot flow is passed on to the system CPU subsystem where the secure CM33 application starts. After all necessary secure configurations, the flow is passed on to the non-secure CM33 application. Resource initialization for this example is performed by this CM33 non-secure project. It configures the system clocks, pins, clock to peripheral connections, and other platform resources. It then enables the CM55 core using the `Cy_SysEnableCM55()` function and puts itself (CM33) into the DeepSleep mode. The main firmware executes on the CM55 core.

In the CM33 non-secure application, the clocks and system resources are initialized by the BSP initialization function. The retarget-io middleware is configured to use the debug UART. The debug UART prints a message (as shown in [Terminal output on program startup](../images/terminal-vision-deploy.png)) on the terminal emulator, the onboard KitProg3 acts the USB-UART bridge to create the virtual COM port. The User LED blinks every 1 second.

Once CM55 is enabled it configures system clocks, pins, clock to peripheral connections, and other platform resources.

The code example is a Rock-Paper-Scissors demo running on a PSOC&trade; Edge MCU, the CM55 application drives the HBVCAM OV7675 0.3MP MINI Camera for real-time image capture, an object detection model for gesture recognition, and a graphics subsystem for displaying results on Waveshare 4.3 inch Raspberry Pi DSI 800*480 display. The graphics subsystem of PSOC&trade; Edge MCU houses an independent 2.5D graphics processing unit (GPU), a display controller (DC), and a MIPI DSI host controller with MIPI D-PHY physical layer interface. The example is built using a FreeRTOS to manage three concurrent tasks: USB webcam handling, object detection inference, and graphics rendering.

The RPS model files in the application is generated using the DEEPCRAFT&trade; studio. It can be replaced with any model files generated by following the steps in the [Generating the model files](#generating-the-model-files) section. 

By default, the CM55 application places the model weights in the SoCMEM for best performance. The files generated by DEEPCRAFT&trade; Model Converter let you place model weights and tensor arena by setting specific defines in *Makefile*. The application *Makefile* then uses `CY_ML_MODEL_MEM` to set the location for the model weights.

Following are the key parts in this code example:

**Application code:** The `main()` function in *proj_cm55* > *main.c* file for the initialization of board support package (BSP), retarget-io for debug UART communication, and the creation of the three FreeRTOS tasks. In the following, each task is described with its design and implementation details.

- Webcam task (`cm55_usb_webcam_task`)

  - The `cm55_usb_webcam_task`, defined in *proj_cm55* > *usb_camera_task.c* file, initializes the USB host stack and manages communication with USB cameras. It registers the video device module with the emUSB-Host stack, detects connected devices, configures video settings, and processes video frames for subsequent inference

  - The task initializes the USB host stack with `USBH_Init()` and registers the video device module using `USBH_VIDEO_Init()`. It also sets up a notification hook (`_cbOnAddRemoveDevice`) to handle dynamic device connection/disconnection events

  - Two additional threads (`USBH_Task` and `USBH_ISRTask`) are created to handle USB host operations and interrupts. A mailbox queue (`_VIDEO_MailBox`) is initialized to receive device notifications, triggering `_OnDevReady()` to process the device index, enabling non-blocking operation

  - Upon detecting a video interface, the task displays device information and configures video control settings. It then reads multiple video frames, storing them in buffers for processing by the inference task

- Inference task (`cm55_inference_task`)

  - The `cm55_inference_task`, defined in *proj_cm55* > *inference_task.c* file initializes and runs an object detection model to identify rock, paper, or scissors gestures from camera-captured images. It handles image preprocessing, model inference, and postprocessing, scaling input images to match the model’s requirements and synchronizing results with the graphics task via a semaphore (`model_semaphore`)

  - The task initializes the object detection model using `IMAI_init()`. It dynamically retrieves and logs the model’s input dimensions (width, height, channels) to ensure compatibility with the camera’s image format

  - The `IMAI_api()` function returns the structure `IMAI_api_def`, which has the parameters detailing the configurations made in the DEEPCRAFT&trade; studio for the RPS model

  - The the latest input image is stored to the input image buffer using the function `getImage()`. The `IMAI_compute()` function, process this input image buffer and  executes model inference. The output buffer `data_out` has the prediction information, which is processed and passed to the `cm55_ns_gfx_task` using the `model_semaphore`

- Graphics task (`cm55_ns_gfx_task`)

  - The `cm55_ns_gfx_task`, defined in *proj_cm55* > *lcd_task.c* file, initializes the graphics subsystem and LCD display driver (e.g., Waveshare 4.3-inch display) to render camera images and object detection results. The task initializes the graphics subsystem with `Cy_GFXSS_Init()` and configures interrupts for the display controller. The I2C interface is set up for display communication

  - A transformation matrix (`matrix`) scales and centers camera images on the display. The task updates the display with inference results (`update_box_data()`) and performance metrics (`Inference_time`) upon receiving a semaphore signal (`model_semaphore`)

  - Double buffering and `vg_lite_flush()` minimize rendering artifacts, while frame rate calculations monitor display performance


### Generating the model files

By default, the code example uses the RPS model that comes with the code example. The model files (`model.c` and `model.h`) generated using the DEEPCRAFT&trade; studio for RPS gesture detection is shipped with the code example. To use a different model, see the [Studio Accelerators examples](https://developer.imagimob.com/deepcraft-studio/getting-started/starter-models-project-templates/starter-models#camera), which is a vision-based from DEEPCRAFT&trade; Studio, or train your own model from scratch by following steps from [Real-Time Image data collection and labeling using camera](https://developer.imagimob.com/deepcraft-studio/data-preparation/data-collection/collect-image-data-using-graph-ux). 

The generated generate `model.c` and `model.h` files, for the target device can be directly imported to the *model* folder in the application. For more information, see the [Code generation for vision models](https://developer.imagimob.com/deepcraft-studio/code-generation/code-gen-vision-models). 

After updating to a newer model rather than the default one used in this example, update the macro `NUM_CLASSES` in the *inference_task.h* file to ensure that the number of classes is in the same order as the one in the DEEPCRAFT&trade; Studio project for the new model. By default, the code example supports maximum five detection at a time. If the number of detection needs to be modified, update the macro `MAX_PREDICTIONS` in the *inference_task.h* file.

<br>